{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sublime-prairie",
   "metadata": {},
   "source": [
    "# Reproducibility as a Service Data Analysis\n",
    "This notebook analyzes the data collected by executing R scripts with and without the retroactive reproducibility tool RaaS.\n",
    "All plots and tables displayed in the paper are generated here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-guide",
   "metadata": {},
   "source": [
    "Imports\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "verbal-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from helper_functions import *\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 25}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-impossible",
   "metadata": {},
   "source": [
    "Analyzing scripts that ran without RaaS\n",
    "=======================================\n",
    "\n",
    "These scripts executed in a rocker/tidyverse environment, R version 3.6.3, with a time limit of one script per hour, up to five hours per dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "julian-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"../data/results.db\")\n",
    "\n",
    "scripts_df = pd.read_sql_query(\"SELECT * FROM results\", con) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accurate-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts_df[\"doi\"] = get_doi_from_results_filename_v(scripts_df[\"filename\"])\n",
    "scripts_df[\"error_category\"] = determine_error_cause_v(scripts_df[\"error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-miami",
   "metadata": {},
   "source": [
    "Comparison of Chen's 2018 Study to our 2022 Study\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-executive",
   "metadata": {},
   "source": [
    "__Comparison of Total Errors and Successes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "scientific-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_scripts = len(scripts_df.index)\n",
    "num_success_scripts = len(scripts_df[scripts_df[\"error_category\"] == \"success\"].index)\n",
    "num_error_scripts = len(scripts_df[scripts_df[\"error_category\"] != \"success\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "flying-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "chen_comparison_markdown = '''\n",
    "------------------------------------------------\n",
    "              Chen's              Ours \n",
    "  --------- -------- --------- ------- ---------\n",
    "  Result       Count   Percent   Count   Percent\n",
    "\n",
    "  Success        408     14.4%   OUR_SUCCESS_COUNT     OUR_SUCCESS_PERCENT%\n",
    "\n",
    "  Error         2431     85.6%   OUR_ERROR_COUNT    OUR_ERROR_PERCENT%\n",
    "\n",
    "  Total         2839             OUR_TOTAL \n",
    "------------------------------------------------\n",
    "\n",
    "\n",
    "Table: The occurrences of errors in scripts from Dataverse without\n",
    "processing through a reproducibility framework. The first set of\n",
    "results are from Chen's study in 2018, and the second is ours\n",
    "conducted in 2022. The percents are rounded to the nearest tenth. {#tbl:error-occurrences}\n",
    "'''\n",
    "\n",
    "chen_comparison_markdown = chen_comparison_markdown.replace(\"OUR_SUCCESS_COUNT\", str(num_success_scripts))\n",
    "chen_comparison_markdown = chen_comparison_markdown.replace(\"OUR_SUCCESS_PERCENT\", \"{0:.4g}\".format(num_success_scripts / total_num_scripts * 100))\n",
    "\n",
    "chen_comparison_markdown = chen_comparison_markdown.replace(\"OUR_ERROR_COUNT\", str(num_error_scripts))\n",
    "chen_comparison_markdown = chen_comparison_markdown.replace(\"OUR_ERROR_PERCENT\", \"{0:.4g}\".format(num_error_scripts / total_num_scripts * 100))\n",
    "\n",
    "chen_comparison_markdown = chen_comparison_markdown.replace(\"OUR_TOTAL\", str(total_num_scripts))\n",
    "\n",
    "write_file_from_string(\"chen_total_comparison.md\", chen_comparison_markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-compilation",
   "metadata": {},
   "source": [
    "__Comparison of Error Categories__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ranking-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_comparison_md = '''\n",
    "------------------------------------------------------------\n",
    "                        containR              RaaS \n",
    "  ------------------- ---------- --------- ------- ---------\n",
    "  Error Type               Count   Percent   Count   Percent\n",
    "\n",
    "  Library                    363     14.9%    LIBRARY_COUNT    LIBRARY_PERCENT%\n",
    "\n",
    "  Working directory          696     28.6%    WD_COUNT    WD_PERCENT%\n",
    "\n",
    "  Missing file               802     33.0%    FILE_COUNT    FILE_PERCENT%\n",
    "\n",
    "  Function                    NA        NA     FUNC_COUNT     FUNC_PERCENT%\n",
    "\n",
    "  Other                      569     23.4%    OTHER_COUNT    OTHER_PERCENT%\n",
    "\n",
    "  Total                     2431              ERROR_TOTAL \n",
    "\n",
    "------------------------------------------------------------\n",
    "\n",
    "Table: The most common causes of errors in scripts from Dataverse without\n",
    "processing through RaaS. The percents are rounded to the nearest\n",
    "tenth. {#tbl:error-causes}\n",
    "'''\n",
    "\n",
    "def replace_in_table(key, category, markdown, scripts_df, total):\n",
    "    error_count = len(scripts_df[scripts_df[\"error_category\"] == category].index)\n",
    "    markdown = markdown.replace(key + \"_COUNT\", str(error_count))\n",
    "    markdown = markdown.replace(key + \"_PERCENT\", \"{0:.4g}\".format(error_count / total * 100))\n",
    "    return(markdown)\n",
    "\n",
    "category_comparison_md = replace_in_table(\"LIBRARY\", \"library\", category_comparison_md, scripts_df, num_error_scripts)\n",
    "category_comparison_md = replace_in_table(\"WD\", \"working directory\", category_comparison_md, scripts_df, num_error_scripts)\n",
    "category_comparison_md = replace_in_table(\"FILE\", \"missing file\", category_comparison_md, scripts_df, num_error_scripts)\n",
    "category_comparison_md = replace_in_table(\"FUNC\", \"function\", category_comparison_md, scripts_df, num_error_scripts)\n",
    "category_comparison_md = replace_in_table(\"OTHER\", \"other\", category_comparison_md, scripts_df, num_error_scripts)\n",
    "\n",
    "category_comparison_md = category_comparison_md.replace(\"ERROR_TOTAL\", str(num_error_scripts))\n",
    "\n",
    "write_file_from_string(\"chen_category_comparison.md\", category_comparison_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-contents",
   "metadata": {},
   "source": [
    "__Breakdown by Subject and Dataset__\n",
    "\n",
    "This section requires the doi_metadata.json file generated by the ``get_doi_metadata.ipynb`` notebook.\n",
    "\n",
    "We'll start by loading metadata scraped from Dataverse, and then converting it into a dataframe that will contain a row for each dataset. To indicate subjects, there will be a set of boolean columns, indicating whether the dataset was in that subject or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "electric-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/doi_metadata.json\", \"r\") as doi_file:\n",
    "    doi_metadata = json.loads(doi_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spoken-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_set = set()\n",
    "for doi_key in doi_metadata:\n",
    "    if doi_metadata[doi_key][0] is not None:\n",
    "        for subject in doi_metadata[doi_key][0]:\n",
    "            subject_set.add(subject)            \n",
    "subject_df = pd.DataFrame(columns = list(subject_set))\n",
    "subject_df.insert(0, \"year\", [])\n",
    "subject_df.insert(0, \"doi\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "greenhouse-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\"doi\": [], \"year\":[]}\n",
    "for subject in subject_set:\n",
    "    df_dict[subject] = []\n",
    "\n",
    "for doi_key in doi_metadata:\n",
    "    if doi_metadata[doi_key][0] is not None:\n",
    "        df_dict[\"year\"].append(doi_metadata[doi_key][1])\n",
    "        df_dict[\"doi\"].append(doi_key.strip(\"\\n\"))\n",
    "        for subject in subject_set:\n",
    "            if subject in doi_metadata[doi_key][0]:\n",
    "                df_dict[subject].append(True)\n",
    "            else:\n",
    "                df_dict[subject].append(False)\n",
    "\n",
    "dataset_df = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "logical-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_clean(doi, scripts_df):\n",
    "    ret_val = None\n",
    "    doi_df = scripts_df[scripts_df[\"doi\"] == doi]\n",
    "    if len(doi_df.index) > 0:\n",
    "        ret_val = False\n",
    "        errors = set(doi_df[\"error\"].values)\n",
    "        if \"success\" in errors and len(errors) == 1:\n",
    "            ret_val = True\n",
    "    return ret_val\n",
    "is_clean_v = np.vectorize(is_clean, excluded=[\"scripts_df\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "objective-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_col = []\n",
    "for doi in dataset_df[\"doi\"].values:\n",
    "    clean_col.append(is_clean(doi, scripts_df))\n",
    "dataset_df[\"clean\"] = clean_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "medieval-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_breakdown_md = '''\n",
    "-------------------------------------------------------------------------------------\n",
    "  Subject                                Total Files   Total Error Files   Error Rate\n",
    "  ------------------------------------ ------------- ------------------- ------------\n",
    "  Social Sciences                              Social Sciences_TOTAL               Social Sciences_ERROR       Social Sciences_PERC%\n",
    "\n",
    "  Computer and Information Science               Computer and Information Science_TOTAL                 Computer and Information Science_ERROR       Computer and Information Science_PERC%\n",
    "\n",
    "  Medicine, Health and Life Sciences             Medicine, Health and Life Sciences_TOTAL                 Medicine, Health and Life Sciences_ERROR       Medicine, Health and Life Sciences_PERC%\n",
    "\n",
    "  Physics                                         Physics_TOTAL                  Physics_ERROR       Physics_PERC%\n",
    "\n",
    "  Engineering                                    Engineering_TOTAL                  Engineering_ERROR       Engineering_PERC%\n",
    "\n",
    "  Other                                          Other_TOTAL                 Other_ERROR       Other_PERC%\n",
    "\n",
    "  Business and Management                        Business and Management_TOTAL                 Business and Management_ERROR        Business and Management_PERC%\n",
    "\n",
    "  Mathematical Sciences                          Mathematical Sciences_TOTAL                 Mathematical Sciences_ERROR       Mathematical Sciences_PERC%\n",
    "\n",
    "  Arts and Humanities                            Arts and Humanities_TOTAL                 Arts and Humanities_ERROR       Arts and Humanities_PERC%\n",
    "\n",
    "  Agricultural Sciences                          Agricultural Sciences_TOTAL                 Agricultural Sciences_ERROR       Agricultural Sciences_PERC%\n",
    "\n",
    "  Law                                            Law_TOTAL                 Law_ERROR       Law_PERC%\n",
    "\n",
    "  Earth and Environmental Sciences               Earth and Environmental Sciences_TOTAL                 Earth and Environmental Sciences_ERROR       Earth and Environmental Sciences_PERC%\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------\n",
    "\n",
    "  Table: This table contains the breakdown of error occurrences in R scripts\n",
    "  on Dataverse by subject. Percentages are rounded to the nearest tenth. {#tbl:subject-breakdown}\n",
    "'''\n",
    "\n",
    "def replace_in_subject_table(key, markdown, subject_script_df):\n",
    "    total = len(subject_script_df.index)\n",
    "    error_count = len(subject_script_df[subject_script_df[\"error_category\"] != \"success\"].index)\n",
    "    markdown = markdown.replace(key + \"_TOTAL\", str(total))\n",
    "    markdown = markdown.replace(key + \"_ERROR\", str(error_count))\n",
    "    markdown = markdown.replace(key + \"_PERC\", \"{0:.4g}\".format(error_count / total * 100))\n",
    "    return(markdown)\n",
    "\n",
    "def get_subject_scripts(subject, dataset_df, scripts_df):\n",
    "    dois_in_subject = dataset_df[dataset_df[subject] == True][\"doi\"].values\n",
    "    scripts_from_doi = scripts_df[scripts_df.doi.isin(dois_in_subject)]\n",
    "    return scripts_from_doi\n",
    "\n",
    "subjects = []\n",
    "for subject in subject_set:\n",
    "    subject_script_df = get_subject_scripts(subject, dataset_df[[\"doi\", subject]], scripts_df)\n",
    "    subject_breakdown_md = replace_in_subject_table(subject, subject_breakdown_md, subject_script_df)\n",
    "\n",
    "write_file_from_string(\"subject_breakdown.md\", subject_breakdown_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-arrival",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-holmes",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
