{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sound-settle",
   "metadata": {},
   "source": [
    "# Code used to generate tables about running R scripts without RaaS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "capable-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-argument",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "The collected data from the evaluation where we tried to execute R scripts without RaaS is stored in the database called 'results.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "close-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"../data/results.db\")\n",
    "\n",
    "errors_df = pd.read_sql_query(\"SELECT * FROM results\", con) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-mambo",
   "metadata": {},
   "source": [
    "## Functions used during analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "apart-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to take the processed filename-friendly version of\n",
    "# a dataset's DOI and return the original DOI\n",
    "def get_doi_from_filename(filename):\n",
    "    filename = filename[2:len(filename)]\n",
    "    doi = os.path.split(filename)[0]\n",
    "    doi = doi.replace(\"-\", \":\", 1)\n",
    "    doi = doi.replace(\"-\", \"/\")\n",
    "    return(doi)\n",
    "\n",
    "# This function categorizes error messages by searching for the most unique and common phrases in different types of R error messages\n",
    "def determine_error_cause(error_msg):\n",
    "    ret_val = \"other\"\n",
    "    \n",
    "    if(\"Error in setwd\" in error_msg):\n",
    "        ret_val = \"working directory\"\n",
    "    elif(\"Error in library\" in error_msg):\n",
    "        ret_val = \"library\"\n",
    "    elif(\"Error in file\" in error_msg):\n",
    "        ret_val = \"missing file\"\n",
    "    elif(\"unable to open\" in error_msg):\n",
    "        ret_val = \"missing file\"\n",
    "    elif(\"Error in readChar\" in error_msg):\n",
    "        ret_val = \"missing file\"\n",
    "    elif(\"could not find function\" in error_msg):\n",
    "        ret_val = \"function\"\n",
    "    elif(\"there is no package called\" in error_msg):\n",
    "        ret_val = \"library\"\n",
    "    elif(\"cannot open the connection\" in error_msg):\n",
    "        ret_val = \"missing file\"\n",
    "        \n",
    "    return(ret_val)\n",
    "\n",
    "# This function is used when viewing the results of this analysis in the notebook\n",
    "def print_error_breakdown(error_type, num_of_errors):\n",
    "    print(error_type + \" errors: \" + str(error_breakdown[error_type]) + \", or \" + str((error_breakdown[error_type] / num_of_errors) * 100) + \"% of total errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-positive",
   "metadata": {},
   "source": [
    "## Analysis \n",
    "\n",
    "This cell analyzes each error stored in the database and uses the previously defined determine_error_cause function to categorize each as either library, working directory, missing file, missing function, or other. It also calculates how many datasets were \"clean,\" meaning all the scripts executed within it ran with no errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "popular-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_error_results ={\"Errors\" : 0, \"No Errors\": 0, \"Clean Datasets\": 0}\n",
    "error_breakdown = {\"library\": 0, \"working directory\": 0, \"missing file\": 0, \"function\":0, \"other\": 0}\n",
    "\n",
    "last_doi = None\n",
    "current_doi_clean = True\n",
    "for index, row in errors_df.iterrows():\n",
    "    doi = get_doi_from_filename(row[\"filename\"])\n",
    "    if(doi != last_doi):\n",
    "        if(current_doi_clean == True):\n",
    "            total_error_results[\"Clean Datasets\"] += 1\n",
    "        current_doi_clean = True\n",
    "    if(row[\"error\"] == \"success\"):\n",
    "        total_error_results[\"No Errors\"] += 1\n",
    "    else:\n",
    "        total_error_results[\"Errors\"] += 1\n",
    "        current_doi_clean = False\n",
    "        error_breakdown[determine_error_cause(row[\"error\"])] += 1\n",
    "        \n",
    "    last_doi = doi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-prague",
   "metadata": {},
   "source": [
    "# View Results\n",
    "\n",
    "This cell prints the results of the evaluation performed checking for errors without RaaS.\n",
    "\n",
    "The publication presents this information in a table generated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "amazing-barcelona",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 10289 total scripts\n",
      "Scripts without errors: 1066, or 10.360579259403247% of total scripts\n",
      "Scripts with errors: 9223, or 89.63942074059675% of total scripts\n",
      "Number of clean datasets: 62 out of 10289\n",
      "##############################################################\n",
      "\n",
      "\n",
      "Error Breakdown: \n",
      "library errors: 5570, or 60.39249701832375% of total errors\n",
      "working directory errors: 1268, or 13.748238100401172% of total errors\n",
      "missing file errors: 854, or 9.25946004553833% of total errors\n",
      "function errors: 614, or 6.657269868806245% of total errors\n",
      "other errors: 917, or 9.9425349669305% of total errors\n"
     ]
    }
   ],
   "source": [
    "total_num_of_scripts = total_error_results[\"Errors\"] + total_error_results[\"No Errors\"]\n",
    "print(\"Out of \" + str(total_num_of_scripts) + \" total scripts\")\n",
    "print(\"Scripts without errors: \" + str(total_error_results['No Errors']) + \", or \" + str((total_error_results['No Errors'] / total_num_of_scripts) * 100) + \"% of total scripts\")\n",
    "print(\"Scripts with errors: \" + str(total_error_results['Errors']) + \", or \" + str((total_error_results['Errors'] / total_num_of_scripts) * 100) + \"% of total scripts\")\n",
    "print(\"Number of clean datasets: \" + str(total_error_results[\"Clean Datasets\"]) + \" out of \" + str(len(errors_df.index)))\n",
    "print(\"##############################################################\\n\\n\")\n",
    "print(\"Error Breakdown: \")\n",
    "print_error_breakdown(\"library\", total_error_results['Errors'])\n",
    "print_error_breakdown(\"working directory\", total_error_results['Errors'])\n",
    "print_error_breakdown(\"missing file\", total_error_results['Errors'])\n",
    "print_error_breakdown(\"function\", total_error_results['Errors'])\n",
    "print_error_breakdown(\"other\", total_error_results['Errors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-grenada",
   "metadata": {},
   "source": [
    "# Generate latex table\n",
    "\n",
    "This cell will take the results computed previously and write the latex table used in the publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "supposed-sugar",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_raas_error_data = []\n",
    "for error_type in error_breakdown.keys():\n",
    "    no_raas_error_data.append({\n",
    "        \"Error Type\": error_type.capitalize(), \n",
    "        \"Count\": error_breakdown[error_type], \n",
    "        \"Percentage (Rounded)\": round((error_breakdown[error_type] / total_error_results['Errors']) * 100, 1),\n",
    "    })\n",
    "    \n",
    "no_raas_error_data_df = pd.DataFrame(no_raas_error_data, columns=[\"Error Type\", \"Count\", \"Percentage (Rounded)\"])\n",
    "\n",
    "with open(\"../results/no_raas_error_data.tex\", \"w\") as no_raas_error_file:\n",
    "    no_raas_error_file.write(no_raas_error_data_df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-salvation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
